Running SLURM prolog script on red121.cluster.local
===============================================================================
Job started on Thu Dec  1 14:43:50 GMT 2022
Job ID          : 2187753
Job name        : trainer
WorkDir         : /mainfs/scratch/mjad1g20/pheno-game/toy-models/stable_baselines
Command         : /mainfs/scratch/mjad1g20/pheno-game/toy-models/stable_baselines/submit.sh
Partition       : batch
Num hosts       : 1
Num cores       : 40
Num of tasks    : 1
Hosts allocated : red121
Job Output Follows ...
===============================================================================
wandb: Tracking run with wandb version 0.13.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./logs/test/TD3_4
Eval num_timesteps=250, episode_reward=0.00 +/- 0.00
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000194 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 0        |
|    precision       | 0.15     |
|    rewards         | 0        |
| time/              |          |
|    total_timesteps | 250      |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.839    |
|    learning_rate   | 0.001    |
|    n_updates       | 200      |
---------------------------------
New best mean reward!
Eval num_timesteps=500, episode_reward=0.00 +/- 0.00
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000129 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 0.00409  |
|    precision       | 0.045    |
|    rewards         | 0.00302  |
| time/              |          |
|    total_timesteps | 500      |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 0.396    |
|    learning_rate   | 0.001    |
|    n_updates       | 400      |
---------------------------------
New best mean reward!
Eval num_timesteps=750, episode_reward=0.00 +/- 0.00
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000259 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 0.00296  |
|    precision       | 0.108    |
|    rewards         | 0.00178  |
| time/              |          |
|    total_timesteps | 750      |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 0.446    |
|    learning_rate   | 0.001    |
|    n_updates       | 600      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 31.2     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 44       |
|    time_elapsed    | 18       |
|    total_timesteps | 800      |
---------------------------------
Eval num_timesteps=1000, episode_reward=2.44 +/- 2.11
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000388 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 2.44     |
|    precision       | 0.064    |
|    rewards         | 1.46     |
| time/              |          |
|    total_timesteps | 1000     |
| train/             |          |
|    actor_loss      | -1.88    |
|    critic_loss     | 0.346    |
|    learning_rate   | 0.001    |
|    n_updates       | 800      |
---------------------------------
New best mean reward!
Eval num_timesteps=1250, episode_reward=5.04 +/- 1.94
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00543  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.04     |
|    precision       | 0.842    |
|    rewards         | 7.18     |
| time/              |          |
|    total_timesteps | 1250     |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 1200     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=1.11 +/- 1.58
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000905 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 1.11     |
|    precision       | 0.238    |
|    rewards         | 2.43     |
| time/              |          |
|    total_timesteps | 1500     |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 0.622    |
|    learning_rate   | 0.001    |
|    n_updates       | 1400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 41.6     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 43       |
|    time_elapsed    | 37       |
|    total_timesteps | 1600     |
---------------------------------
Eval num_timesteps=1750, episode_reward=1.74 +/- 1.33
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00168  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 1.74     |
|    precision       | 0.648    |
|    rewards         | 1.73     |
| time/              |          |
|    total_timesteps | 1750     |
| train/             |          |
|    actor_loss      | -3.71    |
|    critic_loss     | 0.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 1600     |
---------------------------------
Eval num_timesteps=2000, episode_reward=4.11 +/- 2.44
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00821  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.11     |
|    precision       | 0.924    |
|    rewards         | 5.52     |
| time/              |          |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -3.89    |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 1800     |
---------------------------------
Eval num_timesteps=2250, episode_reward=11.90 +/- 3.82
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00343  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 11.9     |
|    precision       | 0.922    |
|    rewards         | 13.7     |
| time/              |          |
|    total_timesteps | 2250     |
| train/             |          |
|    actor_loss      | -3.83    |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.001    |
|    n_updates       | 2200     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 46.5     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 42       |
|    time_elapsed    | 56       |
|    total_timesteps | 2400     |
---------------------------------
Eval num_timesteps=2500, episode_reward=9.96 +/- 1.71
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00317  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 9.96     |
|    precision       | 0.681    |
|    rewards         | 13.2     |
| time/              |          |
|    total_timesteps | 2500     |
| train/             |          |
|    actor_loss      | -4.6     |
|    critic_loss     | 0.926    |
|    learning_rate   | 0.001    |
|    n_updates       | 2400     |
---------------------------------
Eval num_timesteps=2750, episode_reward=0.01 +/- 0.01
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0        |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 0.00615  |
|    precision       | 0        |
|    rewards         | 0.00438  |
| time/              |          |
|    total_timesteps | 2750     |
| train/             |          |
|    actor_loss      | -5.18    |
|    critic_loss     | 1.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 2600     |
---------------------------------
Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.000194 |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 0.000924 |
|    precision       | 0.075    |
|    rewards         | 0.000475 |
| time/              |          |
|    total_timesteps | 3000     |
| train/             |          |
|    actor_loss      | -5.36    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.001    |
|    n_updates       | 2800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 47.2     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 42       |
|    time_elapsed    | 74       |
|    total_timesteps | 3200     |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.865    |
|    learning_rate   | 0.001    |
|    n_updates       | 3000     |
---------------------------------
Eval num_timesteps=3250, episode_reward=6.28 +/- 3.55
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00213  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 6.28     |
|    precision       | 0.491    |
|    rewards         | 5.27     |
| time/              |          |
|    total_timesteps | 3250     |
| train/             |          |
|    actor_loss      | -4.4     |
|    critic_loss     | 0.788    |
|    learning_rate   | 0.001    |
|    n_updates       | 3200     |
---------------------------------
Eval num_timesteps=3500, episode_reward=11.50 +/- 2.09
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00427  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 11.5     |
|    precision       | 0.896    |
|    rewards         | 9.01     |
| time/              |          |
|    total_timesteps | 3500     |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 0.892    |
|    learning_rate   | 0.001    |
|    n_updates       | 3400     |
---------------------------------
Eval num_timesteps=3750, episode_reward=7.04 +/- 0.76
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00582  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 7.04     |
|    precision       | 0.889    |
|    rewards         | 7.66     |
| time/              |          |
|    total_timesteps | 3750     |
| train/             |          |
|    actor_loss      | -5.19    |
|    critic_loss     | 0.992    |
|    learning_rate   | 0.001    |
|    n_updates       | 3600     |
---------------------------------
Eval num_timesteps=4000, episode_reward=6.11 +/- 1.31
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00511  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 6.11     |
|    precision       | 0.885    |
|    rewards         | 7.12     |
| time/              |          |
|    total_timesteps | 4000     |
| train/             |          |
|    actor_loss      | -5.47    |
|    critic_loss     | 1.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 3800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 50.2     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 41       |
|    time_elapsed    | 97       |
|    total_timesteps | 4000     |
---------------------------------
Eval num_timesteps=4250, episode_reward=10.94 +/- 2.28
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00284  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 10.9     |
|    precision       | 0.8      |
|    rewards         | 11.7     |
| time/              |          |
|    total_timesteps | 4250     |
| train/             |          |
|    actor_loss      | -5.99    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 4200     |
---------------------------------
Eval num_timesteps=4500, episode_reward=11.58 +/- 2.09
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00265  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 11.6     |
|    precision       | 0.905    |
|    rewards         | 10.9     |
| time/              |          |
|    total_timesteps | 4500     |
| train/             |          |
|    actor_loss      | -6.4     |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.001    |
|    n_updates       | 4400     |
---------------------------------
Eval num_timesteps=4750, episode_reward=13.93 +/- 1.13
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00278  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 13.9     |
|    precision       | 0.847    |
|    rewards         | 12.6     |
| time/              |          |
|    total_timesteps | 4750     |
| train/             |          |
|    actor_loss      | -6.6     |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.001    |
|    n_updates       | 4600     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 56.3     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 41       |
|    time_elapsed    | 116      |
|    total_timesteps | 4800     |
---------------------------------
Eval num_timesteps=5000, episode_reward=13.96 +/- 2.60
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00375  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 14       |
|    precision       | 0.867    |
|    rewards         | 11.4     |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 4800     |
---------------------------------
New best mean reward!
Eval num_timesteps=5250, episode_reward=63.81 +/- 4.49
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0389   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 63.8     |
|    precision       | 0.937    |
|    rewards         | 62       |
| time/              |          |
|    total_timesteps | 5250     |
| train/             |          |
|    actor_loss      | -7.37    |
|    critic_loss     | 1.3      |
|    learning_rate   | 0.001    |
|    n_updates       | 5200     |
---------------------------------
New best mean reward!
Eval num_timesteps=5500, episode_reward=17.09 +/- 2.74
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00582  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 17.1     |
|    precision       | 0.914    |
|    rewards         | 15.1     |
| time/              |          |
|    total_timesteps | 5500     |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 5400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 62.9     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 41       |
|    time_elapsed    | 135      |
|    total_timesteps | 5600     |
---------------------------------
Eval num_timesteps=5750, episode_reward=14.93 +/- 3.57
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0044   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 14.9     |
|    precision       | 0.894    |
|    rewards         | 19.9     |
| time/              |          |
|    total_timesteps | 5750     |
| train/             |          |
|    actor_loss      | -7.98    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 5600     |
---------------------------------
Eval num_timesteps=6000, episode_reward=14.16 +/- 2.44
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00472  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 14.2     |
|    precision       | 0.881    |
|    rewards         | 15.7     |
| time/              |          |
|    total_timesteps | 6000     |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 5800     |
---------------------------------
Eval num_timesteps=6250, episode_reward=18.05 +/- 3.47
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00485  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 18.1     |
|    precision       | 0.947    |
|    rewards         | 18.9     |
| time/              |          |
|    total_timesteps | 6250     |
| train/             |          |
|    actor_loss      | -8.88    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.001    |
|    n_updates       | 6200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 69.9     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 41       |
|    time_elapsed    | 154      |
|    total_timesteps | 6400     |
---------------------------------
Eval num_timesteps=6500, episode_reward=13.52 +/- 0.24
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00388  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 13.5     |
|    precision       | 0.938    |
|    rewards         | 15.3     |
| time/              |          |
|    total_timesteps | 6500     |
| train/             |          |
|    actor_loss      | -9.15    |
|    critic_loss     | 1.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 6400     |
---------------------------------
Eval num_timesteps=6750, episode_reward=15.35 +/- 3.18
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00485  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 15.3     |
|    precision       | 0.894    |
|    rewards         | 15.1     |
| time/              |          |
|    total_timesteps | 6750     |
| train/             |          |
|    actor_loss      | -9.46    |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 6600     |
---------------------------------
Eval num_timesteps=7000, episode_reward=17.84 +/- 2.49
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00465  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 17.8     |
|    precision       | 0.901    |
|    rewards         | 20.4     |
| time/              |          |
|    total_timesteps | 7000     |
| train/             |          |
|    actor_loss      | -9.73    |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 6800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 74.7     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 41       |
|    time_elapsed    | 174      |
|    total_timesteps | 7200     |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 7000     |
---------------------------------
Eval num_timesteps=7250, episode_reward=23.56 +/- 3.38
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0104   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 23.6     |
|    precision       | 0.957    |
|    rewards         | 23.4     |
| time/              |          |
|    total_timesteps | 7250     |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 7200     |
---------------------------------
Eval num_timesteps=7500, episode_reward=100.30 +/- 8.44
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0458   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 100      |
|    precision       | 0.991    |
|    rewards         | 96.6     |
| time/              |          |
|    total_timesteps | 7500     |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 7400     |
---------------------------------
New best mean reward!
Eval num_timesteps=7750, episode_reward=17.47 +/- 3.89
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00575  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 17.5     |
|    precision       | 0.936    |
|    rewards         | 24.9     |
| time/              |          |
|    total_timesteps | 7750     |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 2.24     |
|    learning_rate   | 0.001    |
|    n_updates       | 7600     |
---------------------------------
Eval num_timesteps=8000, episode_reward=21.10 +/- 6.41
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00537  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 21.1     |
|    precision       | 0.91     |
|    rewards         | 19.9     |
| time/              |          |
|    total_timesteps | 8000     |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 2.35     |
|    learning_rate   | 0.001    |
|    n_updates       | 7800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 78.6     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 40       |
|    time_elapsed    | 197      |
|    total_timesteps | 8000     |
---------------------------------
Eval num_timesteps=8250, episode_reward=20.80 +/- 6.41
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00633  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.8     |
|    precision       | 0.917    |
|    rewards         | 25.1     |
| time/              |          |
|    total_timesteps | 8250     |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 2.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 8200     |
---------------------------------
Eval num_timesteps=8500, episode_reward=25.47 +/- 5.81
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00543  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 25.5     |
|    precision       | 0.898    |
|    rewards         | 18.6     |
| time/              |          |
|    total_timesteps | 8500     |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 2.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 8400     |
---------------------------------
Eval num_timesteps=8750, episode_reward=21.52 +/- 5.88
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00485  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 21.5     |
|    precision       | 0.915    |
|    rewards         | 16.3     |
| time/              |          |
|    total_timesteps | 8750     |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 2.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 8600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 83.2     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 40       |
|    time_elapsed    | 216      |
|    total_timesteps | 8800     |
---------------------------------
Eval num_timesteps=9000, episode_reward=25.03 +/- 4.00
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00627  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 25       |
|    precision       | 0.926    |
|    rewards         | 21.1     |
| time/              |          |
|    total_timesteps | 9000     |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 2.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 8800     |
---------------------------------
Eval num_timesteps=9250, episode_reward=21.76 +/- 1.85
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00465  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 21.8     |
|    precision       | 0.929    |
|    rewards         | 19.1     |
| time/              |          |
|    total_timesteps | 9250     |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 2.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 9200     |
---------------------------------
Eval num_timesteps=9500, episode_reward=20.36 +/- 3.19
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00517  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.4     |
|    precision       | 0.897    |
|    rewards         | 21.8     |
| time/              |          |
|    total_timesteps | 9500     |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 2.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 9400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 86.1     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 40       |
|    time_elapsed    | 235      |
|    total_timesteps | 9600     |
---------------------------------
Eval num_timesteps=9750, episode_reward=19.14 +/- 3.29
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00465  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 19.1     |
|    precision       | 0.894    |
|    rewards         | 15.1     |
| time/              |          |
|    total_timesteps | 9750     |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 3.07     |
|    learning_rate   | 0.001    |
|    n_updates       | 9600     |
---------------------------------
Eval num_timesteps=10000, episode_reward=19.03 +/- 3.66
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0075   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 19       |
|    precision       | 0.927    |
|    rewards         | 19.2     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 3.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 9800     |
---------------------------------
Eval num_timesteps=10250, episode_reward=22.91 +/- 4.69
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00614  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 22.9     |
|    precision       | 0.92     |
|    rewards         | 22.4     |
| time/              |          |
|    total_timesteps | 10250    |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 3.58     |
|    learning_rate   | 0.001    |
|    n_updates       | 10200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 89.2     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 40       |
|    time_elapsed    | 254      |
|    total_timesteps | 10400    |
---------------------------------
Eval num_timesteps=10500, episode_reward=26.27 +/- 5.24
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00511  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 26.3     |
|    precision       | 0.903    |
|    rewards         | 23.6     |
| time/              |          |
|    total_timesteps | 10500    |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 3.35     |
|    learning_rate   | 0.001    |
|    n_updates       | 10400    |
---------------------------------
Eval num_timesteps=10750, episode_reward=18.23 +/- 4.59
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00666  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 18.2     |
|    precision       | 0.918    |
|    rewards         | 21.8     |
| time/              |          |
|    total_timesteps | 10750    |
| train/             |          |
|    actor_loss      | -15.2    |
|    critic_loss     | 3.74     |
|    learning_rate   | 0.001    |
|    n_updates       | 10600    |
---------------------------------
Eval num_timesteps=11000, episode_reward=22.64 +/- 6.31
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00537  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 22.6     |
|    precision       | 0.95     |
|    rewards         | 22.4     |
| time/              |          |
|    total_timesteps | 11000    |
| train/             |          |
|    actor_loss      | -15.5    |
|    critic_loss     | 3.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 10800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 91.3     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 40       |
|    time_elapsed    | 274      |
|    total_timesteps | 11200    |
| train/             |          |
|    actor_loss      | -15.8    |
|    critic_loss     | 3.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 11000    |
---------------------------------
Eval num_timesteps=11250, episode_reward=20.04 +/- 3.56
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00595  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 20       |
|    precision       | 0.94     |
|    rewards         | 20.8     |
| time/              |          |
|    total_timesteps | 11250    |
| train/             |          |
|    actor_loss      | -16      |
|    critic_loss     | 4.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 11200    |
---------------------------------
Eval num_timesteps=11500, episode_reward=22.60 +/- 5.08
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0064   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 22.6     |
|    precision       | 0.954    |
|    rewards         | 23.3     |
| time/              |          |
|    total_timesteps | 11500    |
| train/             |          |
|    actor_loss      | -16.3    |
|    critic_loss     | 3.87     |
|    learning_rate   | 0.001    |
|    n_updates       | 11400    |
---------------------------------
Eval num_timesteps=11750, episode_reward=20.30 +/- 7.27
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00659  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.3     |
|    precision       | 0.931    |
|    rewards         | 22.6     |
| time/              |          |
|    total_timesteps | 11750    |
| train/             |          |
|    actor_loss      | -16.5    |
|    critic_loss     | 4.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 11600    |
---------------------------------
Eval num_timesteps=12000, episode_reward=25.75 +/- 7.74
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00756  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 25.7     |
|    precision       | 0.957    |
|    rewards         | 21       |
| time/              |          |
|    total_timesteps | 12000    |
| train/             |          |
|    actor_loss      | -16.7    |
|    critic_loss     | 3.84     |
|    learning_rate   | 0.001    |
|    n_updates       | 11800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 93.9     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 40       |
|    time_elapsed    | 296      |
|    total_timesteps | 12000    |
---------------------------------
Eval num_timesteps=12250, episode_reward=34.42 +/- 8.56
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0109   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 34.4     |
|    precision       | 0.942    |
|    rewards         | 27.6     |
| time/              |          |
|    total_timesteps | 12250    |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 4.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 12200    |
---------------------------------
Eval num_timesteps=12500, episode_reward=22.29 +/- 6.82
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00834  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 22.3     |
|    precision       | 0.914    |
|    rewards         | 26.2     |
| time/              |          |
|    total_timesteps | 12500    |
| train/             |          |
|    actor_loss      | -17.5    |
|    critic_loss     | 4.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 12400    |
---------------------------------
Eval num_timesteps=12750, episode_reward=41.37 +/- 13.96
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0112   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 41.4     |
|    precision       | 0.964    |
|    rewards         | 25.5     |
| time/              |          |
|    total_timesteps | 12750    |
| train/             |          |
|    actor_loss      | -17.7    |
|    critic_loss     | 4.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 12600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 97       |
| time/              |          |
|    episodes        | 64       |
|    fps             | 40       |
|    time_elapsed    | 315      |
|    total_timesteps | 12800    |
---------------------------------
Eval num_timesteps=13000, episode_reward=34.74 +/- 14.52
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0101   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 34.7     |
|    precision       | 0.95     |
|    rewards         | 29.9     |
| time/              |          |
|    total_timesteps | 13000    |
| train/             |          |
|    actor_loss      | -18      |
|    critic_loss     | 4.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 12800    |
---------------------------------
Eval num_timesteps=13250, episode_reward=30.23 +/- 7.05
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.019    |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 30.2     |
|    precision       | 0.981    |
|    rewards         | 27.9     |
| time/              |          |
|    total_timesteps | 13250    |
| train/             |          |
|    actor_loss      | -18.5    |
|    critic_loss     | 4.76     |
|    learning_rate   | 0.001    |
|    n_updates       | 13200    |
---------------------------------
Eval num_timesteps=13500, episode_reward=36.36 +/- 2.27
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0164   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 36.4     |
|    precision       | 0.962    |
|    rewards         | 33.6     |
| time/              |          |
|    total_timesteps | 13500    |
| train/             |          |
|    actor_loss      | -18.7    |
|    critic_loss     | 4.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 13400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 98.6     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 40       |
|    time_elapsed    | 335      |
|    total_timesteps | 13600    |
---------------------------------
Eval num_timesteps=13750, episode_reward=30.78 +/- 16.27
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.012    |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 30.8     |
|    precision       | 0.958    |
|    rewards         | 31       |
| time/              |          |
|    total_timesteps | 13750    |
| train/             |          |
|    actor_loss      | -18.9    |
|    critic_loss     | 4.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 13600    |
---------------------------------
Eval num_timesteps=14000, episode_reward=32.35 +/- 5.12
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0125   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 32.4     |
|    precision       | 0.962    |
|    rewards         | 27.7     |
| time/              |          |
|    total_timesteps | 14000    |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 4.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 13800    |
---------------------------------
Eval num_timesteps=14250, episode_reward=129.41 +/- 6.42
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0579   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 129      |
|    precision       | 0.994    |
|    rewards         | 137      |
| time/              |          |
|    total_timesteps | 14250    |
| train/             |          |
|    actor_loss      | -19.7    |
|    critic_loss     | 4.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 14200    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 40       |
|    time_elapsed    | 354      |
|    total_timesteps | 14400    |
---------------------------------
Eval num_timesteps=14500, episode_reward=36.54 +/- 14.99
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0154   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 36.5     |
|    precision       | 0.953    |
|    rewards         | 43.5     |
| time/              |          |
|    total_timesteps | 14500    |
| train/             |          |
|    actor_loss      | -19.9    |
|    critic_loss     | 5.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 14400    |
---------------------------------
Eval num_timesteps=14750, episode_reward=129.92 +/- 5.19
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0487   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 130      |
|    precision       | 0.992    |
|    rewards         | 132      |
| time/              |          |
|    total_timesteps | 14750    |
| train/             |          |
|    actor_loss      | -20.1    |
|    critic_loss     | 5.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 14600    |
---------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=129.88 +/- 8.77
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0538   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 130      |
|    precision       | 0.991    |
|    rewards         | 136      |
| time/              |          |
|    total_timesteps | 15000    |
| train/             |          |
|    actor_loss      | -20.4    |
|    critic_loss     | 5.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 14800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 103      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 40       |
|    time_elapsed    | 373      |
|    total_timesteps | 15200    |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 5.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 15000    |
---------------------------------
Eval num_timesteps=15250, episode_reward=29.28 +/- 7.50
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00698  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 29.3     |
|    precision       | 0.952    |
|    rewards         | 28.7     |
| time/              |          |
|    total_timesteps | 15250    |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 5.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 15200    |
---------------------------------
Eval num_timesteps=15500, episode_reward=32.02 +/- 9.78
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00666  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 32       |
|    precision       | 0.942    |
|    rewards         | 29.8     |
| time/              |          |
|    total_timesteps | 15500    |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 5.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 15400    |
---------------------------------
Eval num_timesteps=15750, episode_reward=38.52 +/- 14.66
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00698  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 38.5     |
|    precision       | 0.921    |
|    rewards         | 29.6     |
| time/              |          |
|    total_timesteps | 15750    |
| train/             |          |
|    actor_loss      | -21.4    |
|    critic_loss     | 5.57     |
|    learning_rate   | 0.001    |
|    n_updates       | 15600    |
---------------------------------
Eval num_timesteps=16000, episode_reward=30.66 +/- 8.06
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00743  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 30.7     |
|    precision       | 0.958    |
|    rewards         | 33.6     |
| time/              |          |
|    total_timesteps | 16000    |
| train/             |          |
|    actor_loss      | -21.6    |
|    critic_loss     | 5.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 15800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 40       |
|    time_elapsed    | 396      |
|    total_timesteps | 16000    |
---------------------------------
Eval num_timesteps=16250, episode_reward=35.91 +/- 8.30
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00543  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 35.9     |
|    precision       | 0.943    |
|    rewards         | 26.9     |
| time/              |          |
|    total_timesteps | 16250    |
| train/             |          |
|    actor_loss      | -22.1    |
|    critic_loss     | 5.79     |
|    learning_rate   | 0.001    |
|    n_updates       | 16200    |
---------------------------------
Eval num_timesteps=16500, episode_reward=26.25 +/- 1.35
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00646  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 26.2     |
|    precision       | 0.925    |
|    rewards         | 28.2     |
| time/              |          |
|    total_timesteps | 16500    |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 6.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 16400    |
---------------------------------
Eval num_timesteps=16750, episode_reward=71.32 +/- 14.58
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0154   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 71.3     |
|    precision       | 0.978    |
|    rewards         | 65.2     |
| time/              |          |
|    total_timesteps | 16750    |
| train/             |          |
|    actor_loss      | -22.6    |
|    critic_loss     | 6.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 16600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 40       |
|    time_elapsed    | 416      |
|    total_timesteps | 16800    |
---------------------------------
Eval num_timesteps=17000, episode_reward=34.93 +/- 11.42
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.0116   |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 34.9     |
|    precision       | 0.969    |
|    rewards         | 29.7     |
| time/              |          |
|    total_timesteps | 17000    |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 5.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 16800    |
---------------------------------
Eval num_timesteps=17250, episode_reward=35.39 +/- 7.44
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00718  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 35.4     |
|    precision       | 0.931    |
|    rewards         | 32.8     |
| time/              |          |
|    total_timesteps | 17250    |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 6.78     |
|    learning_rate   | 0.001    |
|    n_updates       | 17200    |
---------------------------------
Eval num_timesteps=17500, episode_reward=29.49 +/- 3.73
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00737  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 29.5     |
|    precision       | 0.98     |
|    rewards         | 31       |
| time/              |          |
|    total_timesteps | 17500    |
| train/             |          |
|    actor_loss      | -23.5    |
|    critic_loss     | 6.25     |
|    learning_rate   | 0.001    |
|    n_updates       | 17400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 40       |
|    time_elapsed    | 435      |
|    total_timesteps | 17600    |
---------------------------------
Eval num_timesteps=17750, episode_reward=32.17 +/- 9.17
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00608  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 32.2     |
|    precision       | 0.947    |
|    rewards         | 30.6     |
| time/              |          |
|    total_timesteps | 17750    |
| train/             |          |
|    actor_loss      | -23.7    |
|    critic_loss     | 7.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 17600    |
---------------------------------
Eval num_timesteps=18000, episode_reward=28.27 +/- 4.53
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00692  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 28.3     |
|    precision       | 0.933    |
|    rewards         | 30.3     |
| time/              |          |
|    total_timesteps | 18000    |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 6.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 17800    |
---------------------------------
Eval num_timesteps=18250, episode_reward=31.43 +/- 7.33
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00698  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 31.4     |
|    precision       | 0.917    |
|    rewards         | 31.9     |
| time/              |          |
|    total_timesteps | 18250    |
| train/             |          |
|    actor_loss      | -24.4    |
|    critic_loss     | 6.85     |
|    learning_rate   | 0.001    |
|    n_updates       | 18200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 40       |
|    time_elapsed    | 454      |
|    total_timesteps | 18400    |
---------------------------------
Eval num_timesteps=18500, episode_reward=29.17 +/- 6.89
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00627  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 29.2     |
|    precision       | 0.941    |
|    rewards         | 29.2     |
| time/              |          |
|    total_timesteps | 18500    |
| train/             |          |
|    actor_loss      | -24.6    |
|    critic_loss     | 7.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 18400    |
---------------------------------
Eval num_timesteps=18750, episode_reward=28.22 +/- 2.40
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00692  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 28.2     |
|    precision       | 0.918    |
|    rewards         | 32.1     |
| time/              |          |
|    total_timesteps | 18750    |
| train/             |          |
|    actor_loss      | -24.8    |
|    critic_loss     | 7.65     |
|    learning_rate   | 0.001    |
|    n_updates       | 18600    |
---------------------------------
Eval num_timesteps=19000, episode_reward=27.93 +/- 4.24
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00621  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 27.9     |
|    precision       | 0.931    |
|    rewards         | 28.6     |
| time/              |          |
|    total_timesteps | 19000    |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 6.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 18800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 40       |
|    time_elapsed    | 473      |
|    total_timesteps | 19200    |
| train/             |          |
|    actor_loss      | -25.3    |
|    critic_loss     | 7.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 19000    |
---------------------------------
Eval num_timesteps=19250, episode_reward=27.89 +/- 3.11
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00692  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 27.9     |
|    precision       | 0.946    |
|    rewards         | 30.2     |
| time/              |          |
|    total_timesteps | 19250    |
| train/             |          |
|    actor_loss      | -25.5    |
|    critic_loss     | 7.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 19200    |
---------------------------------
Eval num_timesteps=19500, episode_reward=29.96 +/- 4.68
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00621  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 30       |
|    precision       | 0.939    |
|    rewards         | 27.9     |
| time/              |          |
|    total_timesteps | 19500    |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 8.92     |
|    learning_rate   | 0.001    |
|    n_updates       | 19400    |
---------------------------------
Eval num_timesteps=19750, episode_reward=31.57 +/- 7.92
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00646  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 31.6     |
|    precision       | 0.961    |
|    rewards         | 28.9     |
| time/              |          |
|    total_timesteps | 19750    |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 7.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 19600    |
---------------------------------
Eval num_timesteps=20000, episode_reward=29.22 +/- 6.20
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    area_covered    | 0.00646  |
|    episode_length  | 200      |
|    mean_ep_length  | 200      |
|    mean_reward     | 29.2     |
|    precision       | 0.919    |
|    rewards         | 28.6     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 8.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 19800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 40       |
|    time_elapsed    | 496      |
|    total_timesteps | 20000    |
---------------------------------
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   eval/area_covered ▁▁▂▁▁▁▁▂▁▁▆▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂█▇▂▂▂▃▂▂▂▂▂▂
wandb: eval/episode_length ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: eval/mean_ep_length ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    eval/mean_reward ▁▁▁▁▂▁▁▁▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃██▃▃▃▅▃▃▃▃▃▃
wandb:      eval/precision ▂▂▇▆▇▁▄▇▇▇█▇█▇██▇▇█▇▇▇█████████▇████▇▇█▇
wandb:        eval/rewards ▁▁▁▁▂▁▁▁▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃██▂▃▂▄▃▃▃▃▃▂
wandb:         global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb: rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: rollout/ep_rew_mean ▁▂▂▂▃▃▄▄▅▅▆▆▆▆▇▇▇▇▇██████
wandb:            time/fps █▆▅▅▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/actor_loss ███▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁
wandb:   train/critic_loss ▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇█
wandb: train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:   eval/area_covered 0.00646
wandb: eval/episode_length 200.0
wandb: eval/mean_ep_length 200.0
wandb:    eval/mean_reward 29.21671
wandb:      eval/precision 0.91942
wandb:        eval/rewards 28.56931
wandb:         global_step 20000
wandb: rollout/ep_len_mean 200.0
wandb: rollout/ep_rew_mean 109.24041
wandb:            time/fps 40.0
wandb:    train/actor_loss -26.12271
wandb:   train/critic_loss 8.15098
wandb: train/learning_rate 0.001
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /mainfs/scratch/mjad1g20/pheno-game/toy-models/stable_baselines/wandb/offline-run-20221201_144516-1ny447q0
wandb: Find logs at: ./wandb/offline-run-20221201_144516-1ny447q0/logs
==============================================================================
Running epilogue script on red121.

Submit time  : 2022-12-01T14:41:29
Start time   : 2022-12-01T14:43:50
End time     : 2022-12-01T14:53:49
Elapsed time : 00:09:59 (Timelimit=01:00:00)

Job ID: 2187753
Cluster: i5
User/Group: mjad1g20/wf
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 40
CPU Utilized: 04:48:45
CPU Efficiency: 72.31% of 06:39:20 core-walltime
Job Wall-clock time: 00:09:59
Memory Utilized: 484.29 MB
Memory Efficiency: 0.30% of 156.25 GB

